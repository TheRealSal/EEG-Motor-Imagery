{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3e476032-ab94-4aec-9397-7267cfd6cf60",
      "metadata": {
        "id": "3e476032-ab94-4aec-9397-7267cfd6cf60"
      },
      "source": [
        "# Multi-Scale Hybrid Neural Networks for EEG Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QcaJEFpX6ER2",
      "metadata": {
        "id": "QcaJEFpX6ER2"
      },
      "source": [
        "**Salman Sami Hussain Ali 40161786**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ihAXub2D75e",
      "metadata": {
        "id": "7ihAXub2D75e"
      },
      "source": [
        "### Abstract\n",
        "The utilization of Motor Imagery (MI) via Electroencephalography (EEG) as a Brain-Computer Interface (BCI) method enables communication with external devices based on the user's brain intentions. While Convolutional Neural Networks (CNNs) have shown promising results in EEG classification tasks, many existing CNN-based approaches rely on a single convolution mode and kernel size. This limitation hampers their ability to efficiently capture multi-scale temporal and spatial features, thereby restricting further enhancements in MI-EEG signal classification accuracy. To address this, my experiments try a novel approach called Multi-Scale Hybrid Convolutional Neural Network (MSHCNN), aimed at enhancing the decoding of MI-EEG signals for improved classification performance. The MSHCNN leverages both two-dimensional and one-dimensional convolutions to extract both temporal and spatial features as well as advanced temporal features from EEG signals.[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XDwiwdhDD_93",
      "metadata": {
        "id": "XDwiwdhDD_93"
      },
      "source": [
        "### Introduction\n",
        "Brain-Computer Interface (BCI) technology has fundamentally transformed our capacity to interact directly with computers through neural activity. By capturing brain signals from various points on the scalp, BCI systems convert these neural impulses into machine-readable commands. This capability circumvents traditional physiological pathways, opening up novel avenues for linking the brain with our technological environment. In simpler terms, brain signals are translated into computer instructions, finding primary utility in medical applications such as assistive technologies, neurorehabilitation, and brain health monitoring.\n",
        "\n",
        "Electroencephalogram (EEG) signals offer a noninvasive means of monitoring brain activity. They capture the electrical signals produced by neurons in the brain using electrodes positioned on the scalp. EEG provides a rich dataset of electrical potentials over time, enabling the study of brain rhythms, event-related potentials, and functional connectivity patterns.\n",
        "\n",
        "Deep neural networks present significant advantages in analyzing neural time series data, particularly EEG signals. These networks can autonomously learn hierarchical representations from raw signals, eliminating the need for manually crafting features, thus saving time. Deep neural networks have demonstrated exceptional performance across various machine learning tasks, including the analysis of neural time series data.\n",
        "\n",
        "Convolutional Neural Networks (CNNs) are particularly favored in neural data analysis due to their optimal decoding performance relative to the number of parameters they require. In EEG decoding, CNNs efficiently extract relevant spatial features from multi-channel EEG data, striking a balance between model complexity and decoding accuracy. Additionally, CNNs offer interpretability through their hierarchical structure and local receptive fields.\n",
        "\n",
        "This preference for CNNs is evident in the current state-of-the-art models for EEG signal processing and BCI applications, as observed through the examination of literature and documentation on EEGNet, ShallowConvNet, and EEGConformer. These models highlight the CNN's ability to achieve an optimal balance between complexity and performance.\n",
        "\n",
        "Two primary training strategies exist for networks dealing with neural time series data: Within-subject and Leave-one-subject-out. Within-subject training involves using data from the same subject for both training and testing. Conversely, Leave-one-subject-out strategy leaves out one subject's data for testing while training on data from the other subjects. This approach aids in learning subject-specific patterns and variations in brain activity across different individuals."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I9V-BiK-EIfN",
      "metadata": {
        "id": "I9V-BiK-EIfN",
        "tags": []
      },
      "source": [
        "### Related Work"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LvG4gZtuETHv",
      "metadata": {
        "id": "LvG4gZtuETHv",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "##### **Setting up Speechbrain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bXqpWzTJRs9IeyWJM8jK1a4l",
      "metadata": {
        "id": "bXqpWzTJRs9IeyWJM8jK1a4l",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/speechbrain/benchmarks.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de24ae23-fba1-4ddb-8814-a6445c4246ef",
      "metadata": {
        "id": "de24ae23-fba1-4ddb-8814-a6445c4246ef",
        "outputId": "7c804a9d-39d2-4722-ea77-aeffb1a1b162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip==22.3.1 in /usr/local/lib/python3.9/dist-packages (22.3.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pip==22.3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f20ff65-3d9c-4a12-9764-25095e0010f3",
      "metadata": {
        "id": "0f20ff65-3d9c-4a12-9764-25095e0010f3"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ac05b8-9b51-4ef3-8289-4d15ba8a0f4e",
      "metadata": {
        "tags": [],
        "id": "32ac05b8-9b51-4ef3-8289-4d15ba8a0f4e"
      },
      "outputs": [],
      "source": [
        "%cd /notebooks/benchmarks\n",
        "!git submodule update --init --recursive\n",
        "%cd /notebooks/benchmarks/speechbrain\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e .\n",
        "%cd /notebooks/benchmarks/benchmarks/MOABB\n",
        "!pip install -r extra-requirements.txt    # Install additional dependencies\n",
        "!pip install -r ../../requirements.txt    # Install base dependencies\n",
        "%cd /notebooks/benchmarks/benchmarks/MOABB\n",
        "%env PYTHON_PATH=/notebooks/benchmarks/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bc38eda-dd1d-46d0-b090-f317749a80c2",
      "metadata": {
        "id": "7bc38eda-dd1d-46d0-b090-f317749a80c2"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y torchaudio torchvision torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d335280-69ff-4760-a9fc-ef3080be4ed2",
      "metadata": {
        "id": "6d335280-69ff-4760-a9fc-ef3080be4ed2"
      },
      "outputs": [],
      "source": [
        "!pip install torchvision torchaudio torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f5894f3-796f-43e6-a846-4901c8be3f48",
      "metadata": {
        "id": "0f5894f3-796f-43e6-a846-4901c8be3f48"
      },
      "outputs": [],
      "source": [
        "!pip uninstall mne --y\n",
        "!pip install mne==1.6.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10338b8c-3fb2-4d2c-9a72-40687ef7577f",
      "metadata": {
        "id": "10338b8c-3fb2-4d2c-9a72-40687ef7577f"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u0wWqxwceB7Y",
      "metadata": {
        "id": "u0wWqxwceB7Y",
        "tags": []
      },
      "source": [
        "### EEGNET"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00TmyXBQu2gZ",
      "metadata": {
        "id": "00TmyXBQu2gZ"
      },
      "source": [
        "EEGNet is currently the state-of-the-art model for EEG classification and interpretation of EEG-based BCI. It is a compact CNN that uses Depthwise and Separable convolutions to construct an EEG-specific network that encapsulate several EEG feature extraction concepts [1]\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1bOwLdmLDMdLyQU70VJVJh2e2AcAIitTp\" width=\"800\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "601c08b1-0373-4047-a814-4f60915859c6",
      "metadata": {
        "id": "601c08b1-0373-4047-a814-4f60915859c6"
      },
      "source": [
        "### ShallowConvNet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583d67a9-bd01-447c-b2fb-b2c4c3751308",
      "metadata": {
        "id": "583d67a9-bd01-447c-b2fb-b2c4c3751308"
      },
      "source": [
        "ShallowConvNet is another state-of-the-art model proposed by Schirrmeister, R.T. et al. for EEG classification and interpretation of EEG-based BCI. It utilizes temporal convolution, followed by spatial convolution across the channels. Then, it pooling is applied to it and it is passed to the classification head[3].\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=13xsTMZ5lqNL0Awdq9YAmtpEjsJgp_5M0\" width=\"800\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2jTVw6rjEhRe",
      "metadata": {
        "id": "2jTVw6rjEhRe",
        "tags": []
      },
      "source": [
        "### Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z-fByhAfrA-y",
      "metadata": {
        "id": "z-fByhAfrA-y"
      },
      "source": [
        "My project mostly consists of my investigation of the two types of convolution utilized in Multi-Scale Hybrid Convolutional Neural Networks(MSHCNN). MSHCNN utilizes a one-dimensional convolution called 1DCNN to extract advanced temporal features of EEG signals, and a two-dimensional convolution called 2DCNN to extract temporal and spatial features of EEG signals(Tang X;Yang C;Sun X;Zou M;Wang H;, 2023). <br><br>In the paper, the novel methodology proposed by Yang, et. Al performed better than EEGNet on certain \"Dataset A\" which consists of EEG signals collected from 9 normal subjects during left-hand and right-hand motor imagery tasks. The data includes recordings from three channels (C3, C4, and Cz) across 5 sessions per subject. They also tested their methodology on \"Dataset B\" which consists of EEG data from 9 normal subjects performing four motor imagery tasks: left hand, right hand, feet, and tongue movements. Each subject participated in two sessions on different days, with each session consisting of 6 cycles. Dataset B is very similar to the BNCI2014001 dataset we have been tasked with. However, for their classification tasks, they took only the left-hand and right-hand motor imagery samples. Furthermore, majority of their reported results cover their model's performance on dataset A, and not B.\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1VlbZ7vXGZi1meEa6YsjKMs-C9mnW6xdA\" width=\"800\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b16d644",
      "metadata": {
        "id": "7b16d644"
      },
      "source": [
        "<br><br>I will be experimenting with the MSHCNN approach, but for all four types of samples: Left-hand, Right-hand, Foot, and Tongue, and comparing its performance with current state-of-the-art models such as EEGNet(Lawhern et al., 2018), and ShallowConvNet(Schirrmeister et al., 2018)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Xx6sMXhRE6u4",
      "metadata": {
        "id": "Xx6sMXhRE6u4"
      },
      "source": [
        "**Intuition behind choosing MSHCNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "899556a0",
      "metadata": {
        "id": "899556a0"
      },
      "source": [
        "I chose MSHCNN because it reported great results for the binary classification between left and right-hand motor imagery, and thought that its level of accuracy could be replicated, but for the complete classification, including tongue and feet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n-UbfcX2E-bJ",
      "metadata": {
        "id": "n-UbfcX2E-bJ"
      },
      "source": [
        "**Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6653a6aa",
      "metadata": {
        "id": "6653a6aa"
      },
      "source": [
        "http://moabb.neurotechx.com/docs/generated/moabb.datasets.BNCI2014_001.html#moabb.datasets.BNCI2014_001\n",
        "\n",
        "The BNCI2014001 dataset contains EEG data from 9 different subjects. Each participant in the study were instructed to imagine performing specific motor tasks corresponding to different classes while their brain activity was recorded. 22 Ag/AgCl electrodes were used for that matter. The four motor imagery tasks are the following:\n",
        "\n",
        "1. Image movement of the left hand (class 1)\n",
        "2. Imagine movement of the right hand (class 2)\n",
        "3. Imagine movement of both feet (class 3)\n",
        "4. Imagine movement of the tongue (class 4)\n",
        "\n",
        "Each subject participated in two sessions on two different days. Each session is comprised of 6 runs separated by short breaks. There are 12 trials for each of the four possible classes, for total of 48 trials in a singular run. Therefore, there are 288 trials per session.\n",
        "\n",
        "During each trial, the subjects are sat in front of a computer screen. At the beginning of the trial, a fixation cross is displayed on the black screen accompanied by a short acoustic warning. After 2 seconds, the first cue in the form of an arrow points to one of the 4 imagery tasks. The cue remains on the screen for 1.25 seconds and the subjects are expected to perform the desired motor imagery task. The cue disappears after the 1.25 seconds and the cross is back on the screen, but the subjects should perform the motor imagery task for 4 seconds, until the cross is no longer displayed. It is important to mention that no feedback is provided during the experiment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MS8dW5buFDlz",
      "metadata": {
        "id": "MS8dW5buFDlz"
      },
      "source": [
        "## Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SqlnMxRwFZL0",
      "metadata": {
        "id": "SqlnMxRwFZL0"
      },
      "source": [
        "#### Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "549dc1c4",
      "metadata": {
        "id": "549dc1c4"
      },
      "source": [
        "I used several types of regularization techniques throughout the model. First, a sequence of batch normalization, dropout, and ReLU activation function is applied on the output of each convolution block in 1DCNN and 2DCNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67IzJsBCFbmO",
      "metadata": {
        "id": "67IzJsBCFbmO"
      },
      "source": [
        "#### Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "432e716d",
      "metadata": {
        "id": "432e716d"
      },
      "source": [
        "For the loss function, I decided to use the same as other models used for the same task: Negative Log Likelihood\n",
        "\n",
        "$$Negative\\space Log\\space Likelihood \\space Loss\\space = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{j=1}^{N} y_{ij} \\log(p_{ij})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nk0WJdVLGlr5",
      "metadata": {
        "id": "nk0WJdVLGlr5",
        "tags": []
      },
      "source": [
        "### MSHCNN: Motor Imagery EEG Decoding Based on Multi-Scale Hybrid Networks and Feature Enhancement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6m9tr-CgFojZ",
      "metadata": {
        "id": "6m9tr-CgFojZ"
      },
      "source": [
        "##### Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de0ce7f",
      "metadata": {
        "id": "0de0ce7f"
      },
      "source": [
        "Model Component Description:\n",
        "1. **Data Input Block**: This is the initial stage where the EEG (Electroencephalography) signals are input into the system. The input data shape for this block is (B, N, T), where B represents the batch size, N represents the number of channels for EEG signals, and T represents the length of the EEG signal.\n",
        "\n",
        "2. **One-Dimensional Multi-Scale Convolutional Neural Network (M1DCNN) Feature Extraction Block**: This block is responsible for extracting temporal features from the EEG signals using a one-dimensional convolutional neural network (1DCNN) on multiple scales. It consists of three 1DCNN blocks and feature splicing layers. The shades of colors in the 1DCNN block represent different convolution kernel sizes.\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1GTTxk3fXzW7g6YLDDv2NHFJzXeogCJXb\" width=\"600\"/>\n",
        "</div>\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=19COWaohJrEOQgtcHKpKfjdzQaI3Y2QRV\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "3. **Two-Dimensional Multi-Scale Convolutional Neural Network (M2DCNN) Feature Extraction Block**: This block operates in parallel with the M1DCNN block to extract spatio-temporal features from the EEG signals using a two-dimensional convolutional neural network (2DCNN) on multiple scales. Similar to M1DCNN, it also has multiple layers for feature extraction, with the shades of colors representing different convolution kernel sizes.\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1RBRfqyogWSlIqQb3tcazQuoDdwy8IwOm\" width=\"600\"/>\n",
        "</div>\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1uRPNCQ5QKnAgmd3VH4E2l088Zm_8Uz34\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "4. **Feature Splicing Block**: This block is responsible for combining the features extracted from both the M1DCNN and M2DCNN blocks. It integrates the temporal and spatial features extracted by the two networks by concatenating over the time axis. Then, the spliced features are subjected to average pooling.\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=192PwJ4esKmY8m2vuNdpxoN4eKawegJEs\" width=\"300\"/>\n",
        "</div>\n",
        "\n",
        "5. **Feature Classification**: After feature extraction and splicing, the resulting features are fed into a classification model for further processing, such as identifying patterns or making predictions based on the EEG signals.\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FitQ9Mrv6Prb4yUk1TmaR3FqcBU2jMQ-\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "**Important:** The passage also highlights that the optimal convolution kernel size for each subject may vary, indicating the importance of adaptability in the convolutional neural network architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38680588",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "38680588"
      },
      "source": [
        "##### PyTorch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2a4ca04-29b6-4620-adea-5d09408dedc6",
      "metadata": {
        "id": "f2a4ca04-29b6-4620-adea-5d09408dedc6",
        "outputId": "06ed8a93-5f16-41b3-82fd-ac498179a37f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /notebooks/benchmarks/benchmarks/MOABB/models/MSHCNN.py\n"
          ]
        }
      ],
      "source": [
        "%%file /notebooks/benchmarks/benchmarks/MOABB/models/MSHCNN.py\n",
        "\n",
        "\"\"\"MSHCNN from https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10036384.\n",
        "Multi-Scale Hybrid convolutional neural networks proposed for a general decoding of single-trial EEG signals.\n",
        "\n",
        "Authors\n",
        " * Salman Sami Hussain Ali, 2024\n",
        "\"\"\"\n",
        "import torch\n",
        "import speechbrain as sb\n",
        "\n",
        "\n",
        "class MSHCNN(torch.nn.Module):\n",
        "    \"\"\"MSHCNN.\n",
        "        Multi-Scale Hybrid Networks\n",
        "        Arguments\n",
        "        ---------\n",
        "        input_shape : tuple\n",
        "            The shape of the input.\n",
        "        one_d_kernel_sizes : list(int)\n",
        "            Kernel sizes for the temporal convolutions in M1DCNN block\n",
        "        two_d_kernel_sizes : list(int)\n",
        "            Kernel sizes for the temporal convolutions in M2DCNN block\n",
        "        postnet_poolsize : tuple\n",
        "            Pool size of the average pooling after M1DCNN and M2DCNN blocks\n",
        "        postnet_poolstride : int\n",
        "            Number of kernels in the 2d spatial depthwise convolution.\n",
        "        temporal_pool_size: tuple\n",
        "            Pool size for M1DCNN and M2DCNN max pooling.\n",
        "        temporal_pool_stride: tuple\n",
        "            Pool stride for M1DCNN and M2DCNN max pooling.\n",
        "        dropout: float\n",
        "            Dropout probability.\n",
        "        dense_n_neurons: int\n",
        "            Number of output neurons.\n",
        "\n",
        "        Example\n",
        "        -------\n",
        "        >>> inp_tensor = torch.rand([1, 200, 32, 1])\n",
        "        >>> model = MSHCNN(input_shape=inp_tensor.shape)\n",
        "        >>> output = model(inp_tensor)\n",
        "        >>> output.shape\n",
        "        torch.Size([1,4])\n",
        "        \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_shape=None,\n",
        "                 one_d_kernel_sizes=[60, 80],\n",
        "                 two_d_kernel_sizes=[60, 80],\n",
        "                 temporal_pool_size=(6, 1),\n",
        "                 temporal_pool_stride=(6, 1),\n",
        "                 postnet_poolsize=8,\n",
        "                 postnet_poolstride=8,\n",
        "                 dropout=0.25,\n",
        "                 dense_n_neurons=4):\n",
        "        super().__init__()\n",
        "\n",
        "        C = input_shape[2]\n",
        "\n",
        "        input_shape_squeezed = input_shape[0:3]\n",
        "\n",
        "        for i in range(len(two_d_kernel_sizes)):\n",
        "            two_d_kernel_sizes[i] = (two_d_kernel_sizes[i], 1)\n",
        "\n",
        "        self.m1dcnn = M1DCNN(\n",
        "            input_shape=input_shape_squeezed,\n",
        "            layers_kernel_sizes=one_d_kernel_sizes,\n",
        "            temporal_pool_size=temporal_pool_size,\n",
        "            temporal_pool_stride=temporal_pool_stride,\n",
        "            dropout=dropout)\n",
        "\n",
        "        self.m2dcnn = M2DCNN(layers_kernel_sizes=two_d_kernel_sizes,\n",
        "                             temporal_pool_size=temporal_pool_size,\n",
        "                             spatial_kernelsize=(1, C),\n",
        "                             temporal_pool_stride=temporal_pool_stride,\n",
        "                             dropout=dropout)\n",
        "\n",
        "        self.pool = sb.nnet.pooling.Pooling1d(\n",
        "            pool_type='avg',\n",
        "            kernel_size=postnet_poolsize,  # (1, kernel_avg_pool),\n",
        "            stride=postnet_poolstride,  # (1, stride_avg_pool),\n",
        "            pool_axis=2,\n",
        "        )\n",
        "\n",
        "        out_m2d = self.m2dcnn(\n",
        "            torch.ones((1,) + tuple(input_shape[1:-1]) + (1,))\n",
        "        ).squeeze(1)\n",
        "\n",
        "        out_m1d = self.m1dcnn(torch.ones((1,) + tuple(input_shape[1:-1]) + (1,)))\n",
        "\n",
        "        out = torch.cat((out_m1d, out_m2d), 1)\n",
        "\n",
        "        dense_input_size = self._num_flat_features(self.pool(out.squeeze(1)))\n",
        "\n",
        "        self.classification = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(dense_input_size, 100),\n",
        "            torch.nn.Linear(100, dense_n_neurons),\n",
        "            torch.nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns the output of the model.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor (batch, time, EEG channel, channel)\n",
        "            Input to convolve. 4d tensors are expected.\n",
        "        \"\"\"\n",
        "        x = x.to(\"cuda\")\n",
        "        m1dcnn = self.m1dcnn(x) # Batch, T_m, Channel(10)\n",
        "\n",
        "        m2dcnn = self.m2dcnn(x)# Batch, 1, T_n, Channel(10)\n",
        "        m2dcnn = m2dcnn.squeeze(1)# Batch, T_n, Channel(10)\n",
        "\n",
        "        res = torch.cat([m1dcnn, m2dcnn], dim=1)# Batch, T_m + T_n, Channel(10)\n",
        "        res = self.pool(res)\n",
        "\n",
        "        return self.classification(res)\n",
        "\n",
        "    def _num_flat_features(self, x):\n",
        "        \"\"\"Returns the number of flattened features from a tensor.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor\n",
        "            Input feature map.\n",
        "        \"\"\"\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "class M1DCNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "            M1DCNN\n",
        "            Arguments\n",
        "            ---------\n",
        "            input_shape : tuple\n",
        "                The shape of the input.\n",
        "            layers_kernel_sizes : list(int)\n",
        "                Kernel sizes for the temporal convolutions in M1DCNN block\n",
        "            temporal_pool_size: int\n",
        "                Pool size for 1DCNN average pooling.\n",
        "            temporal_pool_stride: int\n",
        "                Pool stride for 1DCNN average pooling.\n",
        "            dropout: float\n",
        "                Dropout probability.\n",
        "\n",
        "            Example\n",
        "            -------\n",
        "            >>> inp_tensor = torch.rand([1, 200, 32, 1])\n",
        "            >>> model = MSHCNN(input_shape=inp_tensor.shape, layers_kernel_sizes=[5,10,20], temporal_pool_size=6, temporal_pool_stride=2, dropout=0)\n",
        "            >>> output = model(inp_tensor)\n",
        "            >>> output.shape\n",
        "            torch.Size([1,150, 10])\n",
        "            \"\"\"\n",
        "    def __init__(self, layers_kernel_sizes, temporal_pool_size, dropout, temporal_pool_stride, input_shape):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "\n",
        "        for i in range(len(layers_kernel_sizes)):\n",
        "            layer = OneDCNN(\n",
        "                input_shape=input_shape,\n",
        "                        cnn_temporal_kernelsize=layers_kernel_sizes[i],\n",
        "                        dropout=dropout,\n",
        "                        temporal_pool_size=temporal_pool_size,\n",
        "                        temporal_pool_stride=temporal_pool_stride)\n",
        "            self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns the output of the M1DCNN block.\n",
        "        It returns the concatenation of all the 1DCNN blocks within over the time axis\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor (batch, time, EEG channel, channel)\n",
        "            Input to convolve. 4d tensors are expected.\n",
        "        \"\"\"\n",
        "        concatenated_output = None\n",
        "\n",
        "        for layer in self.layers:\n",
        "            layer_output = layer(x)\n",
        "            if concatenated_output is None:\n",
        "                concatenated_output = layer_output\n",
        "            else:\n",
        "                concatenated_output = torch.cat((concatenated_output, layer_output), dim=1)\n",
        "        return concatenated_output # Batch, T_n, Channel(10)\n",
        "\n",
        "\n",
        "class OneDCNN(torch.nn.Module):\n",
        "    \"\"\"OneDCNN\n",
        "\n",
        "                Arguments\n",
        "                ---------\n",
        "                input_shape : tuple\n",
        "                    The shape of the input.\n",
        "                cnn_temporal_kernelsize : int\n",
        "                    Kernel size for the temporal convolution in 1DCNN block\n",
        "                temporal_pool_size: int\n",
        "                    Pool size for 1DCNN average pooling.\n",
        "                temporal_pool_stride: int\n",
        "                    Pool stride for 1DCNN average pooling.\n",
        "                dropout: float\n",
        "                    Dropout probability.\n",
        "\n",
        "                Example\n",
        "                -------\n",
        "                >>> inp_tensor = torch.rand([1, 200, 32, 1])\n",
        "                >>> model = MSHCNN(input_shape=inp_tensor.shape, layers_kernel_sizes=[5,10,20], temporal_pool_size=6, temporal_pool_stride=2, dropout=0)\n",
        "                >>> output = model(inp_tensor)\n",
        "                >>> output.shape\n",
        "                torch.Size([1,150, 10])\n",
        "                \"\"\"\n",
        "    def __init__(self,\n",
        "                 cnn_temporal_kernelsize,\n",
        "                 dropout,\n",
        "                 temporal_pool_size,\n",
        "                 temporal_pool_stride,\n",
        "                input_shape):\n",
        "        super().__init__()\n",
        "        self.conv1 = sb.nnet.CNN.Conv1d(out_channels=10,\n",
        "                                     kernel_size=cnn_temporal_kernelsize,\n",
        "                                     stride=3,\n",
        "                                     padding=\"valid\",\n",
        "                                     input_shape=input_shape)\n",
        "\n",
        "        self.bn1 = sb.nnet.normalization.BatchNorm1d(\n",
        "            input_size=10, momentum=0.01, affine=True,\n",
        "        )\n",
        "\n",
        "        self.dropout1 = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = torch.nn.Conv1d(10, 10, 3, stride=1, padding=0)\n",
        "\n",
        "        self.bn2 = sb.nnet.normalization.BatchNorm1d(\n",
        "            input_size=10, momentum=0.01, affine=True,\n",
        "        )\n",
        "\n",
        "        self.dropout2 = torch.nn.Dropout(dropout)\n",
        "\n",
        "        self.pooling = sb.nnet.pooling.Pooling1d(\n",
        "            pool_type='max',\n",
        "            kernel_size=6,\n",
        "            stride=6,\n",
        "            pool_axis=2,\n",
        "        )\n",
        "\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns the output of the 1DCNN block.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor (batch, time, EEG channel, channel)\n",
        "            Input to convolve. 4d tensors are expected.\n",
        "        \"\"\"\n",
        "        # You have a tensor with shape (B, Time, Channel, 1)\n",
        "        # Squeeze the tensor to remove the dimension of size 1\n",
        "        x_reshaped = x.squeeze(-1)\n",
        "\n",
        "        out = self.activation(self.dropout1(self.bn1(self.conv1(x_reshaped)))).transpose(1,2)\n",
        "\n",
        "        out = self.activation(self.dropout2(self.bn2(self.conv2(out).transpose(1,2)))).transpose(1,2)\n",
        "\n",
        "        out = self.pooling(out) # Batch, Channel(10), Time\n",
        "        return out.transpose(1,2) # Batch, Channel(10), Time\n",
        "\n",
        "\n",
        "class M2DCNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "                M2DCNN\n",
        "                Arguments\n",
        "                ---------\n",
        "                input_shape : tuple\n",
        "                    The shape of the input.\n",
        "                layers_kernel_sizes : list(int)\n",
        "                    Kernel sizes for the temporal convolutions in M2DCNN block\n",
        "                temporal_pool_size: int\n",
        "                    Pool size for 2DCNN average pooling.\n",
        "                temporal_pool_stride: int\n",
        "                    Pool stride for 2DCNN average pooling.\n",
        "                dropout: float\n",
        "                    Dropout probability.\n",
        "\n",
        "                Example\n",
        "                -------\n",
        "                >>> inp_tensor = torch.rand([1, 200, 32, 1])\n",
        "                >>> model = MSHCNN(input_shape=inp_tensor.shape, layers_kernel_sizes=[5,10,20], temporal_pool_size=6, temporal_pool_stride=2, dropout=0)\n",
        "                >>> output = model(inp_tensor)\n",
        "                >>> output.shape\n",
        "                torch.Size([1,1, 150, 10])\n",
        "                \"\"\"\n",
        "    def __init__(self, layers_kernel_sizes,\n",
        "                 temporal_pool_size,\n",
        "                 temporal_pool_stride,\n",
        "                 dropout,\n",
        "                 spatial_kernelsize=(1, 10),\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "\n",
        "        for i in range(len(layers_kernel_sizes)):\n",
        "            layer = TwoDCNN(\n",
        "                cnn_temporal_kernelsize=layers_kernel_sizes[i],\n",
        "                dropout=dropout,\n",
        "                temporal_pool_size=temporal_pool_size,\n",
        "                cnn_spatial_kernelsize=spatial_kernelsize,\n",
        "                temporal_pool_stride=temporal_pool_stride)\n",
        "            self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns the output of the M2DCNN.\n",
        "        It returns the concatenation of all the 2DCNN blocks within over the time axis\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor (batch, time, EEG channel, channel)\n",
        "            Input to convolve. 4d tensors are expected.\n",
        "        \"\"\"\n",
        "        concatenated_output = None\n",
        "        for layer in self.layers:\n",
        "            layer_output = layer(x) # Batch, 1, Time\n",
        "            layer_output = layer_output.transpose(1,2)# Batch, Time, 1\n",
        "            if concatenated_output is None:\n",
        "                concatenated_output = layer_output # Batch, 1, T_n, Channel(10)\n",
        "            else:\n",
        "                concatenated_output = torch.cat((concatenated_output, layer_output), dim=2)\n",
        "        return concatenated_output\n",
        "\n",
        "\n",
        "class TwoDCNN(torch.nn.Module):\n",
        "    \"\"\"TwoDCNN\n",
        "\n",
        "                    Arguments\n",
        "                    ---------\n",
        "                    cnn_temporal_kernelsize : int\n",
        "                        Kernel size for the temporal convolution in 2DCNN block\n",
        "                    cnn_spatial_kernelsize: tuple\n",
        "                        Kernel size of the 2d spatial convolution.\n",
        "                    temporal_pool_size: int\n",
        "                        Pool size for 2DCNN average pooling.\n",
        "                    temporal_pool_stride: int\n",
        "                        Pool stride for 2DCNN average pooling.\n",
        "                    dropout: float\n",
        "                        Dropout probability.\n",
        "\n",
        "                    Example\n",
        "                    -------\n",
        "                    >>> inp_tensor = torch.rand([1, 200, 32, 1])\n",
        "                    >>> model = MSHCNN(input_shape=inp_tensor.shape, layers_kernel_sizes=[5,10,20], temporal_pool_size=6, temporal_pool_stride=2, dropout=0)\n",
        "                    >>> output = model(inp_tensor)\n",
        "                    >>> output.shape\n",
        "                    torch.Size([1,1,150, 10])\n",
        "                    \"\"\"\n",
        "    def __init__(self,\n",
        "                 cnn_temporal_kernelsize,\n",
        "                 cnn_spatial_kernelsize,\n",
        "                 dropout,\n",
        "                 temporal_pool_size,\n",
        "                 temporal_pool_stride):\n",
        "        super().__init__()\n",
        "\n",
        "        # CONVOLUTIONAL MODULE\n",
        "        self.conv_module = torch.nn.Sequential()\n",
        "        # Temporal convolution\n",
        "        self.conv_module.add_module(\n",
        "            \"conv_0\",\n",
        "            sb.nnet.CNN.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=10,\n",
        "                kernel_size=cnn_temporal_kernelsize,\n",
        "                padding=\"valid\",\n",
        "                bias=True,\n",
        "                swap=True,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"bnorm_1\",\n",
        "            sb.nnet.normalization.BatchNorm2d(\n",
        "                input_size=10, momentum=0.1, affine=True,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"dropout_1\", torch.nn.Dropout(p=dropout),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"relu_1\", torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Spatial convolution\n",
        "        self.conv_module.add_module(\n",
        "            \"conv_1\",\n",
        "            sb.nnet.CNN.Conv2d(\n",
        "                in_channels=10,\n",
        "                out_channels=10,\n",
        "                kernel_size=cnn_spatial_kernelsize,\n",
        "                padding=\"valid\",\n",
        "                bias=False,\n",
        "                swap=True,\n",
        "            ),\n",
        "        )\n",
        "        self.conv_module.add_module(\n",
        "            \"bnorm_2\",\n",
        "            sb.nnet.normalization.BatchNorm2d(\n",
        "                input_size=10, momentum=0.1, affine=True,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"dropout_2\", torch.nn.Dropout(p=dropout),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"relu_2\", torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"pool_1\",\n",
        "            sb.nnet.pooling.Pooling2d(\n",
        "                pool_type='avg',\n",
        "                kernel_size=temporal_pool_size,\n",
        "                stride=temporal_pool_stride,\n",
        "                pool_axis=[1, 2],\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns the output of the 2DCNN block.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor (batch, time, EEG channel, channel)\n",
        "            Input to convolve. 4d tensors are expected.\n",
        "        \"\"\"\n",
        "        result = self.conv_module(x) # Batch, 1, T_n, Channel(10)\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f97fb85-b802-4061-92de-786c969113a4",
      "metadata": {
        "tags": [],
        "id": "8f97fb85-b802-4061-92de-786c969113a4"
      },
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c0943f1-df1e-4bed-99da-5c3f16803928",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "2c0943f1-df1e-4bed-99da-5c3f16803928"
      },
      "source": [
        "##### First Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce1d0e4c-a95a-4245-ad59-b35fea228fae",
      "metadata": {
        "id": "ce1d0e4c-a95a-4245-ad59-b35fea228fae",
        "outputId": "e66d0859-d780-40eb-b96b-e3f6982bd314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /notebooks/benchmarks/benchmarks/MOABB/hparams/MotorImagery/BNCI2014001/MSHCNN_first.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file /notebooks/benchmarks/benchmarks/MOABB/hparams/MotorImagery/BNCI2014001/MSHCNN_first.yaml\n",
        "\n",
        "seed: 1234\n",
        "__set_torchseed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# DIRECTORIES\n",
        "data_folder: !PLACEHOLDER  #'/path/to/dataset'. The dataset will be automatically downloaded in this folder\n",
        "cached_data_folder: !PLACEHOLDER #'path/to/pickled/dataset'\n",
        "output_folder: !PLACEHOLDER #'path/to/results'\n",
        "\n",
        "# DATASET HPARS\n",
        "# Defining the MOABB dataset.\n",
        "dataset: !new:moabb.datasets.BNCI2014001\n",
        "save_prepared_dataset: True # set to True if you want to save the prepared dataset as a pkl file to load and use afterwards\n",
        "data_iterator_name: !PLACEHOLDER\n",
        "target_subject_idx: !PLACEHOLDER\n",
        "target_session_idx: !PLACEHOLDER\n",
        "events_to_load: null # all events will be loaded\n",
        "original_sample_rate: 250 # Original sampling rate provided by dataset authors\n",
        "sample_rate: 125 # Target sampling rate (Hz)\n",
        "# band-pass filtering cut-off frequencies\n",
        "fmin: 0.11 # @orion_step1: --fmin~\"uniform(0.1, 5, precision=2)\"\n",
        "fmax: 50.0 # @orion_step1: --fmax~\"uniform(20.0, 50.0, precision=3)\"\n",
        "n_classes: 4\n",
        "# tmin, tmax respect to stimulus onset that define the interval attribute of the dataset class\n",
        "# trial begins (0 s), cue (2 s, 1.25 s long); each trial is 6 s long\n",
        "# dataset interval starts from 2\n",
        "# -->tmin tmax are referred to this start value (e.g., tmin=0.5 corresponds to 2.5 s)\n",
        "tmin: 0.\n",
        "tmax: 4.0 # @orion_step1: --tmax~\"uniform(1.0, 4.0, precision=2)\"\n",
        "# number of steps used when selecting adjacent channels from a seed channel (default at Cz)\n",
        "n_steps_channel_selection: 2 # @orion_step1: --n_steps_channel_selection~\"uniform(1, 3,discrete=True)\"\n",
        "T: !apply:math.ceil\n",
        "    - !ref <sample_rate> * (<tmax> - <tmin>)\n",
        "C: 22\n",
        "# We here specify how to perfom test:\n",
        "# - If test_with: 'last' we perform test with the latest model.\n",
        "# - if test_with: 'best, we perform test with the best model (according to the metric specified in test_key)\n",
        "# The variable avg_models can be used to average the parameters of the last (or best) N saved models before testing.\n",
        "# This can have a regularization effect. If avg_models: 1, the last (or best) model is used directly.\n",
        "test_with: 'last' # 'last' or 'best'\n",
        "test_key: \"acc\" # Possible opts: \"loss\", \"f1\", \"auc\", \"acc\"\n",
        "\n",
        "# METRICS\n",
        "f1: !name:sklearn.metrics.f1_score\n",
        "    average: 'macro'\n",
        "acc: !name:sklearn.metrics.balanced_accuracy_score\n",
        "cm: !name:sklearn.metrics.confusion_matrix\n",
        "metrics:\n",
        "    f1: !ref <f1>\n",
        "    acc: !ref <acc>\n",
        "    cm: !ref <cm>\n",
        "# TRAINING HPARS\n",
        "n_train_examples: 100  # it will be replaced in the train script\n",
        "# checkpoints to average\n",
        "avg_models: 1 # @orion_step1: --avg_models~\"uniform(1, 15,discrete=True)\"\n",
        "number_of_epochs: 150 # @orion_step1: --number_of_epochs~\"uniform(250, 1000, discrete=True)\"\n",
        "lr: 0.001 # @orion_step1: --lr~\"choices([0.01, 0.005, 0.001, 0.0005, 0.0001])\"\n",
        "# Learning rate scheduling (cyclic learning rate is used here)\n",
        "max_lr: !ref <lr> # Upper bound of the cycle (max value of the lr)\n",
        "base_lr: 0.001 # Lower bound in the cycle (min value of the lr)\n",
        "step_size_multiplier: 5 #from 2 to 8\n",
        "step_size: !apply:round\n",
        "    - !ref <step_size_multiplier> * <n_train_examples> / <batch_size>\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n",
        "    base_lr: !ref <base_lr>\n",
        "    max_lr: !ref <max_lr>\n",
        "    step_size: !ref <step_size>\n",
        "label_smoothing: 0.0\n",
        "loss: !name:speechbrain.nnet.losses.nll_loss\n",
        "    label_smoothing: !ref <label_smoothing>\n",
        "optimizer: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter  # epoch counter\n",
        "    limit: !ref <number_of_epochs>\n",
        "#batch_size_exponent: 6 # @orion_step1: --batch_size_exponent~\"uniform(4, 6,discrete=True)\"\n",
        "batch_size: 20\n",
        "valid_ratio: 0.2\n",
        "\n",
        "# DATA AUGMENTATION\n",
        "# cutcat (disabled when min_num_segments=max_num_segments=1)\n",
        "max_num_segments: 3 # @orion_step2: --max_num_segments~\"uniform(2, 6, discrete=True)\"\n",
        "cutcat: !new:speechbrain.augment.time_domain.CutCat\n",
        "    min_num_segments: 2\n",
        "    max_num_segments: !ref <max_num_segments>\n",
        "# random amplitude gain between 0.5-1.5 uV (disabled when amp_delta=0.)\n",
        "amp_delta: 0.008079 # @orion_step2: --amp_delta~\"uniform(0.0, 0.5)\"\n",
        "rand_amp: !new:speechbrain.augment.time_domain.RandAmp\n",
        "    amp_low: !ref 1 - <amp_delta>\n",
        "    amp_high: !ref 1 + <amp_delta>\n",
        "# random shifts between -300 ms to 300 ms (disabled when shift_delta=0.)\n",
        "shift_delta_: 25 # orion_step2: --shift_delta_~\"uniform(0, 25, discrete=True)\"\n",
        "shift_delta: !ref 1e-2 * <shift_delta_> # 0.250 # 0.-0.25 with steps of 0.01\n",
        "min_shift: !apply:math.floor\n",
        "    - !ref 0 - <sample_rate> * <shift_delta>\n",
        "max_shift: !apply:math.floor\n",
        "    - !ref 0 + <sample_rate> * <shift_delta>\n",
        "time_shift: !new:speechbrain.augment.freq_domain.RandomShift\n",
        "    min_shift: !ref <min_shift>\n",
        "    max_shift: !ref <max_shift>\n",
        "    dim: 1\n",
        "# injection of gaussian white noise\n",
        "snr_white_low: 15.0 # @orion_step2: --snr_white_low~\"uniform(0.0, 15, precision=2)\"\n",
        "snr_white_delta: 5.49 # @orion_step2: --snr_white_delta~\"uniform(5.0, 20.0, precision=3)\"\n",
        "snr_white_high: !ref <snr_white_low> + <snr_white_delta>\n",
        "add_noise_white: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    snr_low: !ref <snr_white_low>\n",
        "    snr_high: !ref <snr_white_high>\n",
        "\n",
        "repeat_augment: 1 # @orion_step1: --repeat_augment 0\n",
        "augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    parallel_augment_fixed_bs: True\n",
        "    repeat_augment: !ref <repeat_augment>\n",
        "    shuffle_augmentations: True\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augmentations: [\n",
        "        !ref <cutcat>,\n",
        "        !ref <rand_amp>,\n",
        "        !ref <time_shift>,\n",
        "        !ref <add_noise_white>]\n",
        "\n",
        "# DATA NORMALIZATION\n",
        "dims_to_normalize: 1 # 1 (time) or 2 (EEG channels)\n",
        "normalize: !name:speechbrain.processing.signal_processing.mean_std_norm\n",
        "    dims: !ref <dims_to_normalize>\n",
        "# MODEL\n",
        "input_shape: [null, !ref <T>, !ref <C>, null]\n",
        "cnn_temporal_kernels: 54 # @orion_step1: --cnn_temporal_kernels~\"uniform(4, 64,discrete=True)\"\n",
        "cnn_spatial_kernels: !ref <cnn_temporal_kernels>\n",
        "cnn_temporal_kernelsize: 6 # @orion_step1: --cnn_temporal_kernelsize~\"uniform(5, 62,discrete=True)\"\n",
        "cnn_temporal_pool_stride: 6\n",
        "\n",
        "# pool size / stride from 4/125 ms to 40/125 ms = circa 30 ms\n",
        "#cnn_poolsize: !ref <cnn_poolsize_> * 4 # same resolution as for EEGNet research space\n",
        "#cnn_poolstride: !ref <cnn_poolstride_> * 4 # same resolution as for EEGNet research space\n",
        "dropout: 0.25 # @orion_step1: --dropout~\"uniform(0.0, 0.5)\"\n",
        "one_d_cnn_temporal_kernels: [40,70,85]\n",
        "two_d_cnn_temporal_kernels: [45,60,90]\n",
        "\n",
        "postnet_poolsize: 8\n",
        "postnet_poolstride: 8\n",
        "\n",
        "model: !new:models.MSHCNN.MSHCNN\n",
        "    input_shape: !ref <input_shape>\n",
        "    one_d_kernel_sizes: !ref <one_d_cnn_temporal_kernels>\n",
        "    two_d_kernel_sizes: !ref <two_d_cnn_temporal_kernels>\n",
        "    temporal_pool_size: [!ref <cnn_temporal_kernelsize>, 1]\n",
        "    temporal_pool_stride: [!ref <cnn_temporal_pool_stride>, 1]\n",
        "    postnet_poolsize: !ref <postnet_poolsize>\n",
        "    postnet_poolstride: !ref <postnet_poolstride>\n",
        "    dropout: !ref <dropout>\n",
        "    dense_n_neurons: !ref <n_classes>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8017144e-3dd7-42a2-93d1-886b87d7f564",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "8017144e-3dd7-42a2-93d1-886b87d7f564"
      },
      "source": [
        "**Training Script**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b48387-6786-4045-bb70-8453337da8a2",
      "metadata": {
        "id": "46b48387-6786-4045-bb70-8453337da8a2"
      },
      "outputs": [],
      "source": [
        "%cd /notebooks/benchmarks/benchmarks/MOABB\n",
        "!./run_experiments.sh --hparams hparams/MotorImagery/BNCI2014001/MSHCNN_first.yaml --data_folder eeg_data --output_folder results/MotorImagery/BNCI2014001/MSHCNN-first --nsbj 9 --nsess 2 --nruns 10 --train_mode leave-one-session-out --device=cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713584cc-3626-4ed6-a5e3-e175ffea0515",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "713584cc-3626-4ed6-a5e3-e175ffea0515"
      },
      "source": [
        "##### Using the Kernel Sizes defined by Jia et al[4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "960362f3-2e80-44a3-9c04-0ca0a8bc9df5",
      "metadata": {
        "id": "960362f3-2e80-44a3-9c04-0ca0a8bc9df5",
        "outputId": "aef3bd09-8d33-4176-b727-a1feaad66c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /notebooks/benchmarks/benchmarks/MOABB/hparams/MotorImagery/BNCI2014001/MSHCNN_first.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file /notebooks/benchmarks/benchmarks/MOABB/hparams/MotorImagery/BNCI2014001/MSHCNN_first.yaml\n",
        "\n",
        "seed: 1234\n",
        "__set_torchseed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# DIRECTORIES\n",
        "data_folder: !PLACEHOLDER  #'/path/to/dataset'. The dataset will be automatically downloaded in this folder\n",
        "cached_data_folder: !PLACEHOLDER #'path/to/pickled/dataset'\n",
        "output_folder: !PLACEHOLDER #'path/to/results'\n",
        "\n",
        "# DATASET HPARS\n",
        "# Defining the MOABB dataset.\n",
        "dataset: !new:moabb.datasets.BNCI2014001\n",
        "save_prepared_dataset: True # set to True if you want to save the prepared dataset as a pkl file to load and use afterwards\n",
        "data_iterator_name: !PLACEHOLDER\n",
        "target_subject_idx: !PLACEHOLDER\n",
        "target_session_idx: !PLACEHOLDER\n",
        "events_to_load: null # all events will be loaded\n",
        "original_sample_rate: 250 # Original sampling rate provided by dataset authors\n",
        "sample_rate: 125 # Target sampling rate (Hz)\n",
        "# band-pass filtering cut-off frequencies\n",
        "fmin: 0.11 # @orion_step1: --fmin~\"uniform(0.1, 5, precision=2)\"\n",
        "fmax: 50.0 # @orion_step1: --fmax~\"uniform(20.0, 50.0, precision=3)\"\n",
        "n_classes: 4\n",
        "# tmin, tmax respect to stimulus onset that define the interval attribute of the dataset class\n",
        "# trial begins (0 s), cue (2 s, 1.25 s long); each trial is 6 s long\n",
        "# dataset interval starts from 2\n",
        "# -->tmin tmax are referred to this start value (e.g., tmin=0.5 corresponds to 2.5 s)\n",
        "tmin: 0.\n",
        "tmax: 4.0 # @orion_step1: --tmax~\"uniform(1.0, 4.0, precision=2)\"\n",
        "# number of steps used when selecting adjacent channels from a seed channel (default at Cz)\n",
        "n_steps_channel_selection: 2 # @orion_step1: --n_steps_channel_selection~\"uniform(1, 3,discrete=True)\"\n",
        "T: !apply:math.ceil\n",
        "    - !ref <sample_rate> * (<tmax> - <tmin>)\n",
        "C: 22\n",
        "# We here specify how to perfom test:\n",
        "# - If test_with: 'last' we perform test with the latest model.\n",
        "# - if test_with: 'best, we perform test with the best model (according to the metric specified in test_key)\n",
        "# The variable avg_models can be used to average the parameters of the last (or best) N saved models before testing.\n",
        "# This can have a regularization effect. If avg_models: 1, the last (or best) model is used directly.\n",
        "test_with: 'last' # 'last' or 'best'\n",
        "test_key: \"acc\" # Possible opts: \"loss\", \"f1\", \"auc\", \"acc\"\n",
        "\n",
        "# METRICS\n",
        "f1: !name:sklearn.metrics.f1_score\n",
        "    average: 'macro'\n",
        "acc: !name:sklearn.metrics.balanced_accuracy_score\n",
        "cm: !name:sklearn.metrics.confusion_matrix\n",
        "metrics:\n",
        "    f1: !ref <f1>\n",
        "    acc: !ref <acc>\n",
        "    cm: !ref <cm>\n",
        "# TRAINING HPARS\n",
        "n_train_examples: 100  # it will be replaced in the train script\n",
        "# checkpoints to average\n",
        "avg_models: 1 # @orion_step1: --avg_models~\"uniform(1, 15,discrete=True)\"\n",
        "number_of_epochs: 100 # @orion_step1: --number_of_epochs~\"uniform(250, 1000, discrete=True)\"\n",
        "lr: 0.001 # @orion_step1: --lr~\"choices([0.01, 0.005, 0.001, 0.0005, 0.0001])\"\n",
        "# Learning rate scheduling (cyclic learning rate is used here)\n",
        "max_lr: !ref <lr> # Upper bound of the cycle (max value of the lr)\n",
        "base_lr: 0.001 # Lower bound in the cycle (min value of the lr)\n",
        "step_size_multiplier: 5 #from 2 to 8\n",
        "step_size: !apply:round\n",
        "    - !ref <step_size_multiplier> * <n_train_examples> / <batch_size>\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n",
        "    base_lr: !ref <base_lr>\n",
        "    max_lr: !ref <max_lr>\n",
        "    step_size: !ref <step_size>\n",
        "label_smoothing: 0.0\n",
        "loss: !name:speechbrain.nnet.losses.nll_loss\n",
        "    label_smoothing: !ref <label_smoothing>\n",
        "optimizer: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter  # epoch counter\n",
        "    limit: !ref <number_of_epochs>\n",
        "batch_size_exponent: 4 # @orion_step1: --batch_size_exponent~\"uniform(4, 6,discrete=True)\"\n",
        "batch_size: !ref 2 ** <batch_size_exponent>\n",
        "valid_ratio: 0.2\n",
        "\n",
        "# DATA AUGMENTATION\n",
        "# cutcat (disabled when min_num_segments=max_num_segments=1)\n",
        "max_num_segments: 3 # @orion_step2: --max_num_segments~\"uniform(2, 6, discrete=True)\"\n",
        "cutcat: !new:speechbrain.augment.time_domain.CutCat\n",
        "    min_num_segments: 2\n",
        "    max_num_segments: !ref <max_num_segments>\n",
        "# random amplitude gain between 0.5-1.5 uV (disabled when amp_delta=0.)\n",
        "amp_delta: 0.008079 # @orion_step2: --amp_delta~\"uniform(0.0, 0.5)\"\n",
        "rand_amp: !new:speechbrain.augment.time_domain.RandAmp\n",
        "    amp_low: !ref 1 - <amp_delta>\n",
        "    amp_high: !ref 1 + <amp_delta>\n",
        "# random shifts between -300 ms to 300 ms (disabled when shift_delta=0.)\n",
        "shift_delta_: 25 # orion_step2: --shift_delta_~\"uniform(0, 25, discrete=True)\"\n",
        "shift_delta: !ref 1e-2 * <shift_delta_> # 0.250 # 0.-0.25 with steps of 0.01\n",
        "min_shift: !apply:math.floor\n",
        "    - !ref 0 - <sample_rate> * <shift_delta>\n",
        "max_shift: !apply:math.floor\n",
        "    - !ref 0 + <sample_rate> * <shift_delta>\n",
        "time_shift: !new:speechbrain.augment.freq_domain.RandomShift\n",
        "    min_shift: !ref <min_shift>\n",
        "    max_shift: !ref <max_shift>\n",
        "    dim: 1\n",
        "# injection of gaussian white noise\n",
        "snr_white_low: 15.0 # @orion_step2: --snr_white_low~\"uniform(0.0, 15, precision=2)\"\n",
        "snr_white_delta: 5.49 # @orion_step2: --snr_white_delta~\"uniform(5.0, 20.0, precision=3)\"\n",
        "snr_white_high: !ref <snr_white_low> + <snr_white_delta>\n",
        "add_noise_white: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    snr_low: !ref <snr_white_low>\n",
        "    snr_high: !ref <snr_white_high>\n",
        "\n",
        "repeat_augment: 1 # @orion_step1: --repeat_augment 0\n",
        "augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    parallel_augment_fixed_bs: True\n",
        "    repeat_augment: !ref <repeat_augment>\n",
        "    shuffle_augmentations: True\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augmentations: [\n",
        "        !ref <cutcat>,\n",
        "        !ref <rand_amp>,\n",
        "        !ref <time_shift>,\n",
        "        !ref <add_noise_white>]\n",
        "\n",
        "# DATA NORMALIZATION\n",
        "dims_to_normalize: 1 # 1 (time) or 2 (EEG channels)\n",
        "normalize: !name:speechbrain.processing.signal_processing.mean_std_norm\n",
        "    dims: !ref <dims_to_normalize>\n",
        "# MODEL\n",
        "input_shape: [null, !ref <T>, !ref <C>, null]\n",
        "cnn_temporal_kernelsize: 6 # @orion_step1: --cnn_temporal_kernelsize~\"uniform(5, 62,discrete=True)\"\n",
        "cnn_temporal_pool_stride: 6\n",
        "\n",
        "# pool size / stride from 4/125 ms to 40/125 ms = circa 30 ms\n",
        "#cnn_poolsize: !ref <cnn_poolsize_> * 4 # same resolution as for EEGNet research space\n",
        "#cnn_poolstride: !ref <cnn_poolstride_> * 4 # same resolution as for EEGNet research space\n",
        "dropout: 0.25 # @orion_step1: --dropout~\"uniform(0.0, 0.5)\"\n",
        "one_d_cnn_temporal_kernels: [15,85]\n",
        "two_d_cnn_temporal_kernels: [45,105]\n",
        "\n",
        "postnet_poolsize: 8\n",
        "postnet_poolstride: 8\n",
        "\n",
        "model: !new:models.MSHCNN.MSHCNN\n",
        "    input_shape: !ref <input_shape>\n",
        "    one_d_kernel_sizes: !ref <one_d_cnn_temporal_kernels>\n",
        "    two_d_kernel_sizes: !ref <two_d_cnn_temporal_kernels>\n",
        "    temporal_pool_size: [!ref <cnn_temporal_kernelsize>, 1]\n",
        "    temporal_pool_stride: [!ref <cnn_temporal_pool_stride>, 1]\n",
        "    postnet_poolsize: !ref <postnet_poolsize>\n",
        "    postnet_poolstride: !ref <postnet_poolstride>\n",
        "    dropout: !ref <dropout>\n",
        "    dense_n_neurons: !ref <n_classes>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e001c235-9022-4b7e-9eca-dc0d9237bcad",
      "metadata": {
        "id": "e001c235-9022-4b7e-9eca-dc0d9237bcad"
      },
      "source": [
        "**Training Script**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f7eeb9-2be2-43dd-88c2-d91c64c8051a",
      "metadata": {
        "tags": [],
        "id": "a9f7eeb9-2be2-43dd-88c2-d91c64c8051a"
      },
      "outputs": [],
      "source": [
        "%cd /notebooks/benchmarks/benchmarks/MOABB\n",
        "!./run_experiments.sh --hparams hparams/MotorImagery/BNCI2014001/MSHCNN_first.yaml --data_folder eeg_data --output_folder results/MotorImagery/BNCI2014001/MSHCNN-first --nsbj 9 --nsess 2 --nruns 10 --train_mode leave-one-session-out --device=cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c766b569-e1ea-4cfc-99ca-ce28fc48791e",
      "metadata": {
        "id": "c766b569-e1ea-4cfc-99ca-ce28fc48791e"
      },
      "source": [
        "**Hyperparameter Tuning:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8267aecd-4e84-41be-a0f6-1924969a11d8",
      "metadata": {
        "tags": [],
        "id": "8267aecd-4e84-41be-a0f6-1924969a11d8"
      },
      "outputs": [],
      "source": [
        "!./run_hparam_optimization.sh --exp_name 'MSHCNN_BNCI2014001_hopt_' \\\n",
        "                             --output_folder results/MotorImagery/BNCI2014001/MSHCNN/hopt \\\n",
        "                             --data_folder eeg_data/ \\\n",
        "                             --hparams hparams/MotorImagery/BNCI2014001/MSHCNN_first.yaml \\\n",
        "                             --nsbj 9 --nsess 2 \\\n",
        "                             --nsbj_hpsearch 9 --nsess_hpsearch 2 \\\n",
        "                             --nruns 1 \\\n",
        "                             --nruns_eval 10 \\\n",
        "                             --eval_metric acc \\\n",
        "                             --train_mode leave-one-session-out \\\n",
        "                             --exp_max_trials 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c162ad98-d937-477c-9638-478afe377859",
      "metadata": {
        "id": "c162ad98-d937-477c-9638-478afe377859"
      },
      "source": [
        "#### HSHCNN Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9eb3f2-7132-4c65-9118-315bd1e79a6f",
      "metadata": {
        "id": "5d9eb3f2-7132-4c65-9118-315bd1e79a6f"
      },
      "source": [
        "After a lot of experimentation, I came to inconclusive results with MSHCNN. It is highly dependent on the subjects and the temporal convolution sizes. There were experiments where the the validation and test accuracy did not exceed 30%, but there were also experiments where the valdiation accuracy reached 80%. The model is extremely sensitive to kernel sizes, and will not converge for many kernel sizes and kernel size combinations. The following screenshot is the result of the hyperparameter tuning done. It should be noted that this accuracy is about half of what was reported in the MSHCNN paper by Tang X;Yang et Al. and this leads me to conclude that the model proposed by them is suitable only for binary classification, not 4 as is tasked in this project. This is because the paper's results are tested on right-hand and left-hand labels only, not including foot and tongue.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1LyfJsFA-Wp7CCJT44TuXdsMemxiZXRak\" width=\"800\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e67c3fed-d879-4bbd-9459-74cf0c35eb99",
      "metadata": {
        "id": "e67c3fed-d879-4bbd-9459-74cf0c35eb99"
      },
      "source": [
        "### M2DCNN Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "841e888b-4c16-41ba-82ff-96ef57757a61",
      "metadata": {
        "id": "841e888b-4c16-41ba-82ff-96ef57757a61"
      },
      "source": [
        "After my experimentation with MSHCNNs, I decided to see what the effect of using strictly M1DCNN and M2DCNN blocks for classification would be like. M2DCNN is very similar to ShallowConvNet in structure, except that it is missing the log and square functions before the classification. So, I decided to add them to M2DCNN and experiment with what is essentially ShallowConvNet, but with different temporal kernel sizes concatenated together.\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "602c1fab-f715-4805-a861-bc08102db6e2",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "602c1fab-f715-4805-a861-bc08102db6e2"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b252c2a-993f-46f3-b8ea-fa6eef25c094",
      "metadata": {
        "id": "5b252c2a-993f-46f3-b8ea-fa6eef25c094",
        "outputId": "3cb2ef65-a4cc-4197-e4e5-95558e8bf155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /notebooks/benchmarks/benchmarks/MOABB/models/M2DCNNClassifier.py\n"
          ]
        }
      ],
      "source": [
        "%%file /notebooks/benchmarks/benchmarks/MOABB/models/M2DCNNClassifier.py\n",
        "\n",
        "\"\"\"\n",
        "Authors\n",
        " * Salman Sami Hussain Ali, 2024\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import speechbrain as sb\n",
        "\n",
        "class M2DCNNClassifier(torch.nn.Module):\n",
        "    \"\"\"M2DCNN Classifer Model.\n",
        "        Experimentation of ShallowConvNet with different temporal kernel sizes concatenated together\n",
        "        Arguments\n",
        "        ---------\n",
        "        input_shape : tuple\n",
        "            The shape of the input.\n",
        "        one_d_kernel_sizes : list(int)\n",
        "            Kernel sizes for the temporal convolutions in M1DCNN block\n",
        "        two_d_kernel_sizes : list(int)\n",
        "            Kernel sizes for the temporal convolutions in M2DCNN block\n",
        "        postnet_poolsize : tuple\n",
        "            Pool size of the average pooling after M1DCNN and M2DCNN blocks\n",
        "        postnet_poolstride : int\n",
        "            Number of kernels in the 2d spatial depthwise convolution.\n",
        "        temporal_pool_size: tuple\n",
        "            Pool size for M1DCNN and M2DCNN max pooling.\n",
        "        temporal_pool_stride: tuple\n",
        "            Pool stride for M1DCNN and M2DCNN max pooling.\n",
        "        dropout: float\n",
        "            Dropout probability.\n",
        "        dense_n_neurons: int\n",
        "            Number of output neurons.\n",
        "\n",
        "        Example\n",
        "        -------\n",
        "        >>> inp_tensor = torch.rand([1, 200, 32, 1])\n",
        "        >>> model = MSHCNN(input_shape=inp_tensor.shape)\n",
        "        >>> output = model(inp_tensor)\n",
        "        >>> output.shape\n",
        "        torch.Size([1,4])\n",
        "        \"\"\"\n",
        "    def __init__(self,\n",
        "                 input_shape=None,\n",
        "                 temporal_kernel_sizes=[60, 80],\n",
        "                 temporal_pool_size=(6, 1),\n",
        "                 temporal_pool_stride=(6, 1),\n",
        "                 postnet_poolsize=8,\n",
        "                 postnet_poolstride=8,\n",
        "                 dropout=0.25,\n",
        "                 dense_n_neurons=4):\n",
        "        super().__init__()\n",
        "\n",
        "        C = input_shape[2]\n",
        "\n",
        "        for i in range(len(temporal_kernel_sizes)):\n",
        "            temporal_kernel_sizes[i] = (temporal_kernel_sizes[i], 1)\n",
        "\n",
        "        self.m2dcnn = Shallow_M2DCNN(layers_kernel_sizes=temporal_kernel_sizes,\n",
        "                             temporal_pool_size=temporal_pool_size,\n",
        "                             spatial_kernelsize=(1, C),\n",
        "                             temporal_pool_stride=temporal_pool_stride,\n",
        "                             dropout=dropout)\n",
        "\n",
        "        self.pool = sb.nnet.pooling.Pooling1d(\n",
        "            pool_type='avg',\n",
        "            kernel_size=postnet_poolsize,  # (1, kernel_avg_pool),\n",
        "            stride=postnet_poolstride,  # (1, stride_avg_pool),\n",
        "            pool_axis=2,\n",
        "        )\n",
        "\n",
        "        out_m2d = self.m2dcnn(\n",
        "            torch.ones((1,) + tuple(input_shape[1:-1]) + (1,))\n",
        "        ).squeeze(1)\n",
        "\n",
        "\n",
        "        dense_input_size = self._num_flat_features(self.pool(out_m2d.squeeze(1)))\n",
        "\n",
        "        self.classification = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(dense_input_size, 100),\n",
        "            torch.nn.Linear(100, dense_n_neurons),\n",
        "            torch.nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns the output of the model.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor (batch, time, EEG channel, channel)\n",
        "            Input to convolve. 4d tensors are expected.\n",
        "        \"\"\"\n",
        "        x = x.to(\"cuda\")\n",
        "\n",
        "        m2dcnn = self.m2dcnn(x)# Batch, 1, T_n, Channel(10)\n",
        "        m2dcnn = m2dcnn.squeeze(1)# Batch, T_n, Channel(10)\n",
        "\n",
        "        res = self.pool(m2dcnn)\n",
        "\n",
        "        return self.classification(res)\n",
        "\n",
        "    def _num_flat_features(self, x):\n",
        "        \"\"\"Returns the number of flattened features from a tensor.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor\n",
        "            Input feature map.\n",
        "        \"\"\"\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "class Shallow_M2DCNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "                M2DCNN altered for ShallowConvNet\n",
        "                Arguments\n",
        "                ---------\n",
        "                input_shape : tuple\n",
        "                    The shape of the input.\n",
        "                layers_kernel_sizes : list(int)\n",
        "                    Kernel sizes for the temporal convolutions in M2DCNN block\n",
        "                temporal_pool_size: int\n",
        "                    Pool size for 2DCNN average pooling.\n",
        "                temporal_pool_stride: int\n",
        "                    Pool stride for 2DCNN average pooling.\n",
        "                dropout: float\n",
        "                    Dropout probability.\n",
        "\n",
        "                Example\n",
        "                -------\n",
        "                >>> inp_tensor = torch.rand([1, 200, 32, 1])\n",
        "                >>> model = MSHCNN(input_shape=inp_tensor.shape, layers_kernel_sizes=[5,10,20], temporal_pool_size=6, temporal_pool_stride=2, dropout=0)\n",
        "                >>> output = model(inp_tensor)\n",
        "                >>> output.shape\n",
        "                torch.Size([1,1, 150, 10])\n",
        "                \"\"\"\n",
        "    def __init__(self, layers_kernel_sizes,\n",
        "                 temporal_pool_size,\n",
        "                 temporal_pool_stride,\n",
        "                 dropout,\n",
        "                 spatial_kernelsize=(1, 10),\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "\n",
        "        for i in range(len(layers_kernel_sizes)):\n",
        "            layer = Shallow_TwoDCNN(\n",
        "                cnn_temporal_kernelsize=layers_kernel_sizes[i],\n",
        "                dropout=dropout,\n",
        "                temporal_pool_size=temporal_pool_size,\n",
        "                cnn_spatial_kernelsize=spatial_kernelsize,\n",
        "                temporal_pool_stride=temporal_pool_stride)\n",
        "            self.layers.append(layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns the output of the M2DCNN.\n",
        "        It returns the concatenation of all the 2DCNN blocks within over the time axis\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor (batch, time, EEG channel, channel)\n",
        "            Input to convolve. 4d tensors are expected.\n",
        "        \"\"\"\n",
        "        concatenated_output = None\n",
        "        for layer in self.layers:\n",
        "            layer_output = layer(x) # Batch, 1, Time\n",
        "            layer_output = layer_output.transpose(1,2)# Batch, Time, 1\n",
        "            if concatenated_output is None:\n",
        "                concatenated_output = layer_output # Batch, 1, T_n, Channel(10)\n",
        "            else:\n",
        "                concatenated_output = torch.cat((concatenated_output, layer_output), dim=2)\n",
        "        return concatenated_output\n",
        "\n",
        "\n",
        "class Shallow_TwoDCNN(torch.nn.Module):\n",
        "    \"\"\"TwoDCNN altered for ShallowConvNet\n",
        "\n",
        "                    Arguments\n",
        "                    ---------\n",
        "                    cnn_temporal_kernelsize : int\n",
        "                        Kernel size for the temporal convolution in 2DCNN block\n",
        "                    cnn_spatial_kernelsize: tuple\n",
        "                        Kernel size of the 2d spatial convolution.\n",
        "                    temporal_pool_size: int\n",
        "                        Pool size for 2DCNN average pooling.\n",
        "                    temporal_pool_stride: int\n",
        "                        Pool stride for 2DCNN average pooling.\n",
        "                    dropout: float\n",
        "                        Dropout probability.\n",
        "\n",
        "                    Example\n",
        "                    -------\n",
        "                    >>> inp_tensor = torch.rand([1, 200, 32, 1])\n",
        "                    >>> model = MSHCNN(input_shape=inp_tensor.shape, layers_kernel_sizes=[5,10,20], temporal_pool_size=6, temporal_pool_stride=2, dropout=0)\n",
        "                    >>> output = model(inp_tensor)\n",
        "                    >>> output.shape\n",
        "                    torch.Size([1,1,150, 10])\n",
        "                    \"\"\"\n",
        "    def __init__(self,\n",
        "                 cnn_temporal_kernelsize,\n",
        "                 cnn_spatial_kernelsize,\n",
        "                 dropout,\n",
        "                 temporal_pool_size,\n",
        "                 temporal_pool_stride):\n",
        "        super().__init__()\n",
        "\n",
        "        # CONVOLUTIONAL MODULE\n",
        "        self.conv_module = torch.nn.Sequential()\n",
        "        # Temporal convolution\n",
        "        self.conv_module.add_module(\n",
        "            \"conv_0\",\n",
        "            sb.nnet.CNN.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=10,\n",
        "                kernel_size=cnn_temporal_kernelsize,\n",
        "                padding=\"valid\",\n",
        "                bias=True,\n",
        "                swap=True,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"bnorm_1\",\n",
        "            sb.nnet.normalization.BatchNorm2d(\n",
        "                input_size=10, momentum=0.1, affine=True,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"dropout_1\", torch.nn.Dropout(p=dropout),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"relu_1\", torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Spatial convolution\n",
        "        self.conv_module.add_module(\n",
        "            \"conv_1\",\n",
        "            sb.nnet.CNN.Conv2d(\n",
        "                in_channels=10,\n",
        "                out_channels=10,\n",
        "                kernel_size=cnn_spatial_kernelsize,\n",
        "                padding=\"valid\",\n",
        "                bias=False,\n",
        "                swap=True,\n",
        "            ),\n",
        "        )\n",
        "        self.conv_module.add_module(\n",
        "            \"bnorm_2\",\n",
        "            sb.nnet.normalization.BatchNorm2d(\n",
        "                input_size=10, momentum=0.1, affine=True,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"dropout_2\", torch.nn.Dropout(p=dropout),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"relu_2\", torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"square_1\", Square(),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"pool_1\",\n",
        "            sb.nnet.pooling.Pooling2d(\n",
        "                pool_type='avg',\n",
        "                kernel_size=temporal_pool_size,\n",
        "                stride=temporal_pool_stride,\n",
        "                pool_axis=[1, 2],\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.conv_module.add_module(\n",
        "            \"log_1\", Log(),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Returns the output of the 2DCNN block.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x : torch.Tensor (batch, time, EEG channel, channel)\n",
        "            Input to convolve. 4d tensors are expected.\n",
        "        \"\"\"\n",
        "        result = self.conv_module(x) # Batch, 1, T_n, Channel(10)\n",
        "        return result\n",
        "\n",
        "\n",
        "class Square(torch.nn.Module):\n",
        "    \"\"\"Layer for squaring activations.\"\"\"\n",
        "    def forward(self, x):\n",
        "        return torch.square(x)\n",
        "\n",
        "\n",
        "class Log(torch.nn.Module):\n",
        "    \"\"\"Layer to compute log of activations.\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.log(torch.clamp(x, min=1e-6))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ae09b2-c486-4a9b-87ba-14040d25441c",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "81ae09b2-c486-4a9b-87ba-14040d25441c"
      },
      "source": [
        "#### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db045f0-967f-423a-adda-b0c71019f409",
      "metadata": {
        "id": "6db045f0-967f-423a-adda-b0c71019f409",
        "outputId": "8a5b58f5-2b88-4b79-ba3a-430e5fece71a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /notebooks/benchmarks/benchmarks/MOABB/hparams/MotorImagery/BNCI2014001/M2DCNNClassifier_first.yaml\n"
          ]
        }
      ],
      "source": [
        "%%file /notebooks/benchmarks/benchmarks/MOABB/hparams/MotorImagery/BNCI2014001/M2DCNNClassifier_first.yaml\n",
        "\n",
        "seed: 1234\n",
        "__set_torchseed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# DIRECTORIES\n",
        "data_folder: !PLACEHOLDER  #'/path/to/dataset'. The dataset will be automatically downloaded in this folder\n",
        "cached_data_folder: !PLACEHOLDER #'path/to/pickled/dataset'\n",
        "output_folder: !PLACEHOLDER #'path/to/results'\n",
        "\n",
        "# DATASET HPARS\n",
        "# Defining the MOABB dataset.\n",
        "dataset: !new:moabb.datasets.BNCI2014001\n",
        "save_prepared_dataset: True # set to True if you want to save the prepared dataset as a pkl file to load and use afterwards\n",
        "data_iterator_name: !PLACEHOLDER\n",
        "target_subject_idx: !PLACEHOLDER\n",
        "target_session_idx: !PLACEHOLDER\n",
        "events_to_load: null # all events will be loaded\n",
        "original_sample_rate: 250 # Original sampling rate provided by dataset authors\n",
        "sample_rate: 125 # Target sampling rate (Hz)\n",
        "# band-pass filtering cut-off frequencies\n",
        "fmin: 0.11 # @orion_step1: --fmin~\"uniform(0.1, 5, precision=2)\"\n",
        "fmax: 50.0 # @orion_step1: --fmax~\"uniform(20.0, 50.0, precision=3)\"\n",
        "n_classes: 4\n",
        "# tmin, tmax respect to stimulus onset that define the interval attribute of the dataset class\n",
        "# trial begins (0 s), cue (2 s, 1.25 s long); each trial is 6 s long\n",
        "# dataset interval starts from 2\n",
        "# -->tmin tmax are referred to this start value (e.g., tmin=0.5 corresponds to 2.5 s)\n",
        "tmin: 0.\n",
        "tmax: 4.0 # @orion_step1: --tmax~\"uniform(1.0, 4.0, precision=2)\"\n",
        "# number of steps used when selecting adjacent channels from a seed channel (default at Cz)\n",
        "n_steps_channel_selection: 2 # @orion_step1: --n_steps_channel_selection~\"uniform(1, 3,discrete=True)\"\n",
        "T: !apply:math.ceil\n",
        "    - !ref <sample_rate> * (<tmax> - <tmin>)\n",
        "C: 22\n",
        "# We here specify how to perfom test:\n",
        "# - If test_with: 'last' we perform test with the latest model.\n",
        "# - if test_with: 'best, we perform test with the best model (according to the metric specified in test_key)\n",
        "# The variable avg_models can be used to average the parameters of the last (or best) N saved models before testing.\n",
        "# This can have a regularization effect. If avg_models: 1, the last (or best) model is used directly.\n",
        "test_with: 'last' # 'last' or 'best'\n",
        "test_key: \"acc\" # Possible opts: \"loss\", \"f1\", \"auc\", \"acc\"\n",
        "\n",
        "# METRICS\n",
        "f1: !name:sklearn.metrics.f1_score\n",
        "    average: 'macro'\n",
        "acc: !name:sklearn.metrics.balanced_accuracy_score\n",
        "cm: !name:sklearn.metrics.confusion_matrix\n",
        "metrics:\n",
        "    f1: !ref <f1>\n",
        "    acc: !ref <acc>\n",
        "    cm: !ref <cm>\n",
        "# TRAINING HPARS\n",
        "n_train_examples: 100  # it will be replaced in the train script\n",
        "# checkpoints to average\n",
        "avg_models: 1 # @orion_step1: --avg_models~\"uniform(1, 15,discrete=True)\"\n",
        "number_of_epochs: 100 # @orion_step1: --number_of_epochs~\"uniform(250, 1000, discrete=True)\"\n",
        "lr: 0.001 # @orion_step1: --lr~\"choices([0.01, 0.005, 0.001, 0.0005, 0.0001])\"\n",
        "# Learning rate scheduling (cyclic learning rate is used here)\n",
        "max_lr: !ref <lr> # Upper bound of the cycle (max value of the lr)\n",
        "base_lr: 0.001 # Lower bound in the cycle (min value of the lr)\n",
        "step_size_multiplier: 5 #from 2 to 8\n",
        "step_size: !apply:round\n",
        "    - !ref <step_size_multiplier> * <n_train_examples> / <batch_size>\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n",
        "    base_lr: !ref <base_lr>\n",
        "    max_lr: !ref <max_lr>\n",
        "    step_size: !ref <step_size>\n",
        "label_smoothing: 0.0\n",
        "loss: !name:speechbrain.nnet.losses.nll_loss\n",
        "    label_smoothing: !ref <label_smoothing>\n",
        "optimizer: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter  # epoch counter\n",
        "    limit: !ref <number_of_epochs>\n",
        "batch_size_exponent: 4 # @orion_step1: --batch_size_exponent~\"uniform(4, 6,discrete=True)\"\n",
        "batch_size: !ref 2 ** <batch_size_exponent>\n",
        "valid_ratio: 0.2\n",
        "\n",
        "# DATA AUGMENTATION\n",
        "# cutcat (disabled when min_num_segments=max_num_segments=1)\n",
        "max_num_segments: 3 # @orion_step2: --max_num_segments~\"uniform(2, 6, discrete=True)\"\n",
        "cutcat: !new:speechbrain.augment.time_domain.CutCat\n",
        "    min_num_segments: 2\n",
        "    max_num_segments: !ref <max_num_segments>\n",
        "# random amplitude gain between 0.5-1.5 uV (disabled when amp_delta=0.)\n",
        "amp_delta: 0.008079 # @orion_step2: --amp_delta~\"uniform(0.0, 0.5)\"\n",
        "rand_amp: !new:speechbrain.augment.time_domain.RandAmp\n",
        "    amp_low: !ref 1 - <amp_delta>\n",
        "    amp_high: !ref 1 + <amp_delta>\n",
        "# random shifts between -300 ms to 300 ms (disabled when shift_delta=0.)\n",
        "shift_delta_: 25 # orion_step2: --shift_delta_~\"uniform(0, 25, discrete=True)\"\n",
        "shift_delta: !ref 1e-2 * <shift_delta_> # 0.250 # 0.-0.25 with steps of 0.01\n",
        "min_shift: !apply:math.floor\n",
        "    - !ref 0 - <sample_rate> * <shift_delta>\n",
        "max_shift: !apply:math.floor\n",
        "    - !ref 0 + <sample_rate> * <shift_delta>\n",
        "time_shift: !new:speechbrain.augment.freq_domain.RandomShift\n",
        "    min_shift: !ref <min_shift>\n",
        "    max_shift: !ref <max_shift>\n",
        "    dim: 1\n",
        "# injection of gaussian white noise\n",
        "snr_white_low: 15.0 # @orion_step2: --snr_white_low~\"uniform(0.0, 15, precision=2)\"\n",
        "snr_white_delta: 5.49 # @orion_step2: --snr_white_delta~\"uniform(5.0, 20.0, precision=3)\"\n",
        "snr_white_high: !ref <snr_white_low> + <snr_white_delta>\n",
        "add_noise_white: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    snr_low: !ref <snr_white_low>\n",
        "    snr_high: !ref <snr_white_high>\n",
        "\n",
        "repeat_augment: 1 # @orion_step1: --repeat_augment 0\n",
        "augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    parallel_augment_fixed_bs: True\n",
        "    repeat_augment: !ref <repeat_augment>\n",
        "    shuffle_augmentations: True\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augmentations: [\n",
        "        !ref <cutcat>,\n",
        "        !ref <rand_amp>,\n",
        "        !ref <time_shift>,\n",
        "        !ref <add_noise_white>]\n",
        "\n",
        "# DATA NORMALIZATION\n",
        "dims_to_normalize: 1 # 1 (time) or 2 (EEG channels)\n",
        "normalize: !name:speechbrain.processing.signal_processing.mean_std_norm\n",
        "    dims: !ref <dims_to_normalize>\n",
        "# MODEL\n",
        "input_shape: [null, !ref <T>, !ref <C>, null]\n",
        "cnn_temporal_kernelsize: 6 # @orion_step1: --cnn_temporal_kernelsize~\"uniform(5, 62,discrete=True)\"\n",
        "cnn_temporal_pool_stride: 6\n",
        "\n",
        "# pool size / stride from 4/125 ms to 40/125 ms = circa 30 ms\n",
        "#cnn_poolsize: !ref <cnn_poolsize_> * 4 # same resolution as for EEGNet research space\n",
        "#cnn_poolstride: !ref <cnn_poolstride_> * 4 # same resolution as for EEGNet research space\n",
        "dropout: 0.25 # @orion_step1: --dropout~\"uniform(0.0, 0.5)\"\n",
        "temporal_kernels: [65,45,105]\n",
        "\n",
        "postnet_poolsize: 8\n",
        "postnet_poolstride: 8\n",
        "\n",
        "model: !new:models.M2DCNNClassifier.M2DCNNClassifier\n",
        "    input_shape: !ref <input_shape>\n",
        "    temporal_kernel_sizes: !ref <temporal_kernels>\n",
        "    temporal_pool_size: [!ref <cnn_temporal_kernelsize>, 1]\n",
        "    temporal_pool_stride: [!ref <cnn_temporal_pool_stride>, 1]\n",
        "    postnet_poolsize: !ref <postnet_poolsize>\n",
        "    postnet_poolstride: !ref <postnet_poolstride>\n",
        "    dropout: !ref <dropout>\n",
        "    dense_n_neurons: !ref <n_classes>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e97218a8-6ef5-48d9-b95b-71e44c0eb825",
      "metadata": {
        "id": "e97218a8-6ef5-48d9-b95b-71e44c0eb825"
      },
      "source": [
        "#### Training Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3df38539-7c7d-4f52-97a8-3d9d391b08aa",
      "metadata": {
        "tags": [],
        "id": "3df38539-7c7d-4f52-97a8-3d9d391b08aa"
      },
      "outputs": [],
      "source": [
        "%cd /notebooks/benchmarks/benchmarks/MOABB\n",
        "!./run_experiments.sh --hparams hparams/MotorImagery/BNCI2014001/M2DCNNClassifier_first.yaml --data_folder eeg_data --output_folder results/MotorImagery/BNCI2014001/M2DCNNClassifier_first --nsbj 9 --nsess 2 --nruns 10 --train_mode leave-one-session-out --device=cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RVKy1SpQGDna",
      "metadata": {
        "id": "RVKy1SpQGDna",
        "tags": []
      },
      "source": [
        "#### M2DCNN Classifier Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kbuDMpG0GE4I",
      "metadata": {
        "id": "kbuDMpG0GE4I"
      },
      "source": [
        "This experiment was not succesful as the model was not converging whatsoever. After trying many different kernel sizes and combinations, it failed to converge and accuracy remained at a stable 25%."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YnBeQ6CFGREH",
      "metadata": {
        "id": "YnBeQ6CFGREH",
        "tags": []
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xq836T_rGTTD",
      "metadata": {
        "id": "xq836T_rGTTD"
      },
      "source": [
        "Multi-Scale Hybrid Neural Networks for classification of Motor Imagery has not been very successful for the classification task given. The authors of the MSHCNN paper reported great success with binary classification; between left and right-hand. However, this success did not seem to carry over into the classification of 4 different labels. It might also be worth noting that I attempted to experiment with Mamba as Self-Attention, taking inspiration from EEGConformer, but failed to integrate it with SpeechBrain. Thank you for reading."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29kxejQhGUYl",
      "metadata": {
        "id": "29kxejQhGUYl"
      },
      "source": [
        "## References"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zyKcrWvmGWAw",
      "metadata": {
        "id": "zyKcrWvmGWAw"
      },
      "source": [
        "\n",
        "\n",
        "1.   Lawhern, V.J. et al. (2018) EEGNet: A compact convolutional network for EEG-based brain-computer interfaces, arXiv.org. Available at: https://arxiv.org/abs/1611.08024 (Accessed: 06 April 2024).\n",
        "2.   Tang X;Yang C;Sun X;Zou M;Wang H; (2023) Motor imagery EEG decoding based on multi-scale hybrid networks and feature enhancement, IEEE transactions on neural systems and rehabilitation engineering : a publication of the IEEE Engineering in Medicine and Biology Society. Available at: https://pubmed.ncbi.nlm.nih.gov/37022411/ (Accessed: 06 April 2024).\n",
        "3.  Schirrmeister, R.T. et al. (2018) Deep learning with convolutional neural networks for EEG decoding and visualization, arXiv.org. Available at: https://arxiv.org/abs/1703.05051 (Accessed: 07 April 2024).\n",
        "4.  Z. Jia, Y. Lin, J. Wang, K. Yang, T. Liu, and X. Zhang, “Mmcnn:A multi-branch multi-scale convolutional neural network for motor imagery classification,” in Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discovery Databases. Cham, Switzerland: Springer, 2020, pp. 736–751. [Online]. Available: https://link.springer.com/chapter/10.1007/978-3-030-67664-3_44\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}